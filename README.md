# MLLM-HRC: Multimodal Language Models for Human-Robot Collaboration

An interactive visualization of research on empowering natural human-robot collaboration through multimodal language models and spatial intelligence.

## Features
- Interactive perception-cognition-actuation loop visualization
- Timeline of spatial intelligence pathways
- Real-world application demonstrations
- Responsive design with smooth animations

## Live Demo
[View Project](https://yourusername.github.io/mllm-hrc/)

## Research Paper
Based on "Empowering natural humanâ€“robot collaboration through multimodal language models and spatial intelligence: Pathways and perspectives"

## Technologies Used
- HTML5
- CSS3 (with animations and gradients)
- Vanilla JavaScript
- Responsive Web Design
